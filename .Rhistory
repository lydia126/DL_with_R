# 5.1.8 Predict and Return Tidy Data (MODIFIED)
# Make Predictions
pred_out <- model %>%
predict(x_test_arr, batch_size = batch_size) %>%
.[,1]
# Make future index using tk_make_future_timeseries()
idx <- data %>%
tk_index() %>%
tk_make_future_timeseries(n_future = lag_setting)
# Retransform values
pred_tbl <- tibble(
index   = idx,
value   = (pred_out * scale_history + center_history)^2
)
# Combine actual data with predictions
tbl_1 <- df %>%
add_column(key = "actual")
tbl_3 <- pred_tbl %>%
add_column(key = "predict")
# Create time_bind_rows() to solve dplyr issue
time_bind_rows <- function(data_1, data_2, index) {
index_expr <- enquo(index)
bind_rows(data_1, data_2) %>%
as_tbl_time(index = !! index_expr)
}
ret <- list(tbl_1, tbl_3) %>%
reduce(time_bind_rows, index = index) %>%
arrange(key, index) %>%
mutate(key = as_factor(key))
return(ret)
}
safe_lstm <- possibly(lstm_prediction, otherwise = NA)
safe_lstm(data, epochs, ...)
}
future_sun_spots_tbl <- predict_keras_lstm_future(sun_spots, epochs = 300)
future_sun_spots_tbl %>%
filter_time("1900" ~ "end") %>%
plot_prediction(id = NULL, alpha = 0.4, size = 1.5) +
theme(legend.position = "bottom") +
ggtitle("Sunspots: Ten Year Forecast", subtitle = "Forecast Horizon: 2013 - 2023")
# Deep learing with R Chapter 6
# Local : D:\GitHub\DL_with_R
# Remote: https://github.com/lydia126/DL_with_R
library(keras)
max_features <- 10000
maxlen <- 500
batch_size <- 32
cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb
cat(length(input_train), "train sequences\n")
cat(length(input_test), "test sequences")
cat("Pad sequences (samples x time)\n")
input_train <- pad_sequences(input_train, maxlen = maxlen)
input_test <- pad_sequences(input_test, maxlen = maxlen)
cat("input_train shape:", dim(input_train), "\n")
cat("input_test shape:", dim(input_test), "\n")
model <- keras_model_sequential() %>%
layer_embedding(input_dim = max_features, output_dim = 32) %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history <- model %>% fit(
input_train, y_train,
epochs = 10,
batch_size = 128,
validation_split = 0.2
)
.libPaths()
R_LIBS_SITE = "C:/Users/schun/AppData/Local/R/win-library/4.2"
library(tidyverse)
library(glue)
library(forcats)
# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)
# Visualization
library(cowplot)
# Preprocessing
library(recipes)
# Sampling / Accuracy
library(rsample)
library(yardstick)
# Modeling
library(keras)
# 5.1.6 Building The LSTM Model
model <- keras_model_sequential()
sun_spots <- datasets::sunspot.month %>%
tk_tbl() %>%
mutate(index = as_date(index)) %>%
as_tbl_time(index = index)
sun_spots     #sunspot data from 1749-01 to 2013-09 (n=3177)
summary(sun_spots)
plot(sun_spots)
acf(sun_spots)
spec=spectrum(sun_spots,spans=c(5,5))
spec=spectrum(sun_spots[,2],spans=c(5,5))
spec$freq[which.max(spec$spec)]
sun_spots <- datasets::sunspot.month %>%
tk_tbl() %>%
mutate(index = as_date(index)) %>%
as_tbl_time(index = index)
sun_spots     #sunspot data from 1749-01 to 2013-09 (n=3177)
summary(sun_spots)
plot(value~index,type="l",main="monthly amout of sunspots")
plot(sun_spots$value~sun_spots$index,type="l",main="monthly amout of sunspots")
acf(sun_spots)
acf(sun_spots[,2])
spec=spectrum(sun_spots[,2],spans=c(5,5))
acf(sun_spots[1:3069,2])
spec=spectrum(sun_spots[1:3069,2],spans=c(5,5))
spec$freq[which.max(spec$spec)]
data <- sun_spots[1:3069,]
summary(data)
plot(data$value~data$index,type="l",main="monthly amout of sunspots")
acf(data[1:3069,2])
spec=spectrum(data[,2],spans=c(5,5))   #from 1749-2012
spec$freq[which.max(spec$spec)]   #128 month period
index=seq(from=1, to=length(data))
lo=loess(data$value~data$index,span=0.5)
num.low <- ts(loess(data$value~data$index,span=0.5)$fitted,frequency=128)
num.hi <- ts(num-loess(data$value~data$index,span=0.05)$fitted,frequency=128)
num.cycles <- num - num.hi - num.low
plot(ts.union(num, num.low,num.hi,num.cycles),
main="Decomposition of sunspots amount as trend + noise + cycles")
# Decomposition
index=seq(from=1, to=length(data))
# Decomposition
index=seq(from=1, to=length(data[,2]))
length(data[,2])
length(data)
length(data[,2])
index=seq(from=1, to=length(data[,2]))
lo=loess(data$value~index,span=0.5)
num.low <- ts(loess(data$value~index,span=0.5)$fitted,frequency=128)
num.hi <- ts(num-loess(data$value~index,span=0.05)$fitted,frequency=128)
num.cycles <- num - num.hi - num.low
plot(ts.union(num, num.low,num.hi,num.cycles),
main="Decomposition of sunspots amount as trend + noise + cycles")
# Decomposition
index=seq(from=1, to=length(data)
# Decomposition
index=seq(from=1, to=length(data))
# Decomposition
index=seq(from=1, to=length(data))
# Decomposition
index=seq(from=1, to=3069)
lo=loess(data$value~index,span=0.5)
num.low <- ts(loess(data$value~index,span=0.5)$fitted,frequency=128)
num.hi <- ts(num-loess(data$value~index,span=0.05)$fitted,frequency=128)
num.cycles <- num - num.hi - num.low
plot(ts.union(num, num.low,num.hi,num.cycles),
main="Decomposition of sunspots amount as trend + noise + cycles")
decomp<-decompose(data$value)
plot(decomp)
# Decomposition
decomp<-decompose(data)
plot(data)
data <- sun_spots[1:3168,]
# Decomposition
decomp<-decompose(data)
length(data)
# 2.0 Data
sun_spots <- datasets::sunspot.month
sun_spots     #sunspot data from 1749-01 to 2013-09 (n=3177)
sun_spots <- datasets::sunspot.month %>%
tk_tbl() %>%
mutate(index = as_date(index)) %>%
as_tbl_time(index = index)
sun_spots     #sunspot data from 1749-01 to 2013-09 (n=3177)
data <- sun_spots[1:3168,]
summary(data)
data <- sun_spots[1:3168,2]
summary(data)
year <- seq(from=1749,length=length(data),by=1/12)
plot(data~year,type="l",main="monthly amout of sunspots for 264 years")
sun_spots <- datasets::sunspot.month %>%
tk_tbl() %>%
mutate(index = as_date(index)) %>%
as_tbl_time(index = index)
sun_spots     #sunspot data from 1749-01 to 2013-09 (n=3177)
data <- as.dataframe(sun_spots[1:3168,2])
data <- as.data.frame(sun_spots[1:3168,2])
summary(data)
year <- seq(from=1749,length=length(data), by=1/12)
plot(data~year,type="l",main="monthly amout of sunspots for 264 years")
View(data)
length
plot(data,type="l",main="monthly amout of sunspots for 264 years")
acf(data[,2])
acf(data)
plot(sun_spots,type="l",main="monthly amout of sunspots for 264 years")
spec=spectrum(data,spans=c(5,5))   #from 1749-2012
spec$freq[which.max(spec$spec)]   #128 month period
# Decomposition
index=seq(from=1, to=length(data))
lo=loess(data~index,span=0.5)
length(data)
# 2.0 Data
raw_data <- datasets::sunspot.month
decompose(raw_data)
# Decomposition
decompsed_sun_spots <- decompose(raw_data[1:3168],2)
# Decomposition
decompsed_sun_spots <- decompose(raw_data[1:3168],)
# Decomposition
decompsed_sun_spots <- decompose(raw_data)
decompsed_sun_spots
plot(decompsed_sun_spots)
install.packages("forecast")
# Decomposition
library(forecast)
raw_data_decomp = ts(raw_data, frequency = 12)
decompsed_sun_spots <- decompose(raw_data_decomp, "additive")
plot(decompsed_sun_spots)
plot(decompsed_sun_spots)
raw_data_decomp = ts(raw_data, frequency = 128)
decompsed_sun_spots <- decompose(raw_data_decomp, "additive")
plot(decompsed_sun_spots)
raw_data_decomp = ma(raw_data, order = 12, centre = T)
plot(as.ts(raw_data_decomp))
lines(raw_data_decomp)
plot(as.ts(raw_data_decomp))
raw_data_decomp = ma(raw_data, order = 128, centre = T)
plot(as.ts(raw_data_decomp))
lines(raw_data_decomp)
plot(as.ts(raw_data_decomp))
aic_table <- function(data,P,Q){
table <- matrix(NA,(P+1),(Q+1))
for(p in 0:P) {
for(q in 0:Q) {
table[p+1,q+1] <- arima(data, xreg = index,order=c(p,0,q))$aic
}
}
dimnames(table) <- list(paste("<b> AR",0:P, "</b>", sep=""),paste("MA",0:Q,sep=""))
table
}
price_aic_table <- aic_table(num,4,4)
require(knitr)
kable(price_aic_table,digits=2)
aic_table <- function(data,P,Q){
table <- matrix(NA,(P+1),(Q+1))
for(p in 0:P) {
for(q in 0:Q) {
table[p+1,q+1] <- arima(data, xreg = index,order=c(p,0,q))$aic
}
}
dimnames(table) <- list(paste("<b> AR",0:P, "</b>", sep=""),paste("MA",0:Q,sep=""))
table
}
price_aic_table <- aic_table(num,4,4)
arma21 <- arima(data, , xreg = index,order = c(1,0,2));arma21
arma21 <- arima(data, , xreg = index,order = c(1,0,2));arma21
# 2.0 Data
sunspot <- read.csv("Sunspots.csv")
glimpse(sunspot)
sun_clean <- sunspot %>%
select(-X) %>%
mutate(Date = ymd(Date)) %>%
thicken(interval ="month", colname = "yearmonth") %>% # to convert "daily" observations to monthly observations as there is only one day per month
select(-"Date") %>%  # remove the original date column to use the yearmonth column as the new date column as basis for padding
pad(interval = "month")
anyNA(sun_clean)
install.packages("padr")
# 2.0 Data
library(padr)
sun_clean <- sunspot %>%
select(-X) %>%
mutate(Date = ymd(Date)) %>%
thicken(interval ="month", colname = "yearmonth") %>% # to convert "daily" observations to monthly observations as there is only one day per month
select(-"Date") %>%  # remove the original date column to use the yearmonth column as the new date column as basis for padding
pad(interval = "month")
anyNA(sun_clean)
sunspot_ts <- ts(data=sun_clean$Monthly.Mean.Total.Sunspot.Number,
start = c(1749,1),
frequency = 12)
autoplot(sunspot_ts)
autoplot(decompose(sunspot_ts))
sunspot_11yr <- ts(data=sun_clean$Monthly.Mean.Total.Sunspot.Number,
start = c(1749,1),
frequency = 12*11)
autoplot(decompose(sunspot_11yr))
install.packages("msts")
library(msts)
sunspot_11yr_22yr <- msts(data = sun_clean$Monthly.Mean.Total.Sunspot.Number, seasonal.periods = c(12*11, 12*22))
sunspot_11yr_22yr %>% mstl() %>% autoplot()
sunspot_11yr_22yr_70yr <- msts(data = sun_clean$Monthly.Mean.Total.Sunspot.Number, seasonal.periods = c(12*11, 12*22, 12*70))
sunspot_11yr_22yr_70yr %>% mstl() %>% autoplot()
sunspot_11yr_22yr_train <- sunspot_11yr_22yr %>%  head(-(12*11))
sunspot_11yr_22yr_test <- sunspot_11yr_22yr %>% tail(12*11)
model_11yr_22yr <- sunspot_11yr_22yr_train %>% stlm(
method = "arima")
mdl_11yr_22yr_f <- forecast(model_11yr_22yr, h=12*11)
accuracy(mdl_11yr_22yr_f$mean, sunspot_11yr_22yr_test)
autoplot(sunspot_11yr_22yr %>% tail(12*11*5))+
autolayer(mdl_11yr_22yr_f$mean, series = "prediction")+
autolayer(sunspot_11yr_22yr_test, series = "actual test data")
sunspot_11yr_22yr_70yr_train <- sunspot_11yr_22yr_70yr %>%  head(-(11*12))
sunspot_11yr_22yr_70yr_test <- sunspot_11yr_22yr_70yr %>% tail(11*12)
model_11yr_22yr_70yr <- sunspot_11yr_22yr_70yr_train %>% stlm(
method = "arima")
mdl_11yr_22yr_70yr_f <- forecast(model_11yr_22yr_70yr, h=12*11)
accuracy(mdl_11yr_22yr_70yr_f$mean, sunspot_11yr_22yr_70yr_test)
autoplot(sunspot_11yr_22yr_70yr %>% tail(12*11*5))+
autolayer(mdl_11yr_22yr_70yr_f$mean, series = "prediction")+
autolayer(sunspot_11yr_22yr_70yr_test, series = "actual test data")
newdata <- sunspot_11yr_22yr %>% tail(14*11*12)
autoplot(mstl(newdata))
newdata_train <- newdata %>% head(-(11*12))
newdata_test <- newdata %>% tail(11*12)
model2_11yr_22yr <- newdata_train %>% stlm(
method = "arima")
mdl2_11yr_22yr_f <- forecast(model2_11yr_22yr, h=11*12)
accuracy(mdl2_11yr_22yr_f$mean, newdata_test)
autoplot(newdata %>% tail(12*11*5))+
autolayer(mdl2_11yr_22yr_f$mean, series = "prediction")+
autolayer(newdata_test, series = "actual test data")
sunspot_11yr_22yr_train2 <- sunspot_11yr_22yr %>%  head(-(12))
sunspot_11yr_22yr_test2 <- sunspot_11yr_22yr %>% tail(12)
model3_11yr_22yr <- sunspot_11yr_22yr_train2 %>% stlm(
method = "arima")
mdl3_11yr_22yr_f <- forecast(model3_11yr_22yr, h=12*11)
accuracy(mdl_11yr_22yr_f$mean, sunspot_11yr_22yr_test2)
#7 Assumptions check
Box.test(model_11yr_22yr$residuals, type = "Ljung-Box")
shapiro.test(model_11yr_22yr$residuals)
hist(model_11yr_22yr$residuals, breaks=50)
length(model_11yr_22yr$residuals)
library(tibble)
library(readr)
data_dir <- "D:/GitHub/DL_with_R/data/jena_climate"
fname <- file.path(data_dir, "jena_climate_2009_2016.csv")
data <- read_csv(fname)
glimpse(data)
data <- data.matrix(data[,-1])
# Listing 6.32 Normalizing the data
train_data <- data[1:200000,]
mean <-apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <-scale(data, center = mean, scale = std)
#Listing 6.34 Preparing the training, validation, and test generators
library(keras)
model <- keras_model_sequential() %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu",
input_shape = list(NULL, dim(data)[[-1]])) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
layer_global_max_pooling_1d() %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
generator <- function(data, lookback, delay, min_index, max_index,
shuffle = FALSE, batch_size = 128, step = 6) {
if (is.null(max_index)) max_index <- nrow(data) - delay - 1
i <- min_index + lookback
function() {
if (shuffle) {
rows <- sample(c((min_index+lookback):max_index), size = batch_size)
} else {
if (i + batch_size >= max_index)
i <<- min_index + lookback
rows <- c(i:min(i+batch_size, max_index))
i <<- i + length(rows)
}
samples <- array(0, dim = c(length(rows),
lookback / step,
dim(data)[[-1]]))
targets <- array(0, dim = c(length(rows)))
for (j in 1:length(rows)) {
indices <- seq(rows[[j]] - lookback, rows[[j]],
length.out = dim(samples)[[2]])
samples[j,,] <- data[indices,]
targets[[j]] <- data[rows[[j]] + delay,2]    # targets is temperature (2)
}
list(samples, targets)
}
}
train_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = 200000,
shuffle = TRUE,
step = step,
batch_size = batch_size
)
val_gen = generator(
data,
lookback = lookback,
delay = delay,
min_index = 200001,
max_index = 300000,
step = step,
batch_size = batch_size
)
test_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 300001,
max_index = NULL,
step = step,
batch_size = batch_size
)
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128
train_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = 200000,
shuffle = TRUE,
step = step,
batch_size = batch_size
)
val_gen = generator(
data,
lookback = lookback,
delay = delay,
min_index = 200001,
max_index = 300000,
step = step,
batch_size = batch_size
)
test_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 300001,
max_index = NULL,
step = step,
batch_size = batch_size
)
history <- model %>% fit(
train_gen,
steps_per_epoch = 500,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
)
val_steps <- (300000 - 200001 - lookback) / batch_size
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
history <- model %>% fit(
train_gen,
steps_per_epoch = 500,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
)
step <- 3
lookback <- 720
delay <- 144
train_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = 200000,
shuffle = TRUE,
step = step
)
val_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 200001,
max_index = 300000,
step = step
)
test_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 300001,
max_index = NULL,
step = step
)
val_steps <- (300000 - 200001 - lookback) / 128
test_steps <- (nrow(data) - 300001 - lookback) / 128
model <- keras_model_sequential() %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu",
input_shape = list(NULL, dim(data)[[-1]])) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
layer_gru(units = 32, dropout = 0.1, recurrent_dropout = 0.5) %>%
layer_dense(units = 1)
summary(model)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
history <- model %>% fit_generator(
train_gen,
steps_per_epoch = 500,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
)
plot(history)
